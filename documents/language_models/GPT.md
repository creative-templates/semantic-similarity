# Generative pre-trained transformer (GPT) Language Model

Generative pre-trained transformers (GPT) are a family of large language models (LLMs), which was introduced in 2018 by the American artificial intelligence organization OpenAI. GPT models are artificial neural networks that are based on the transformer architecture, pre-trained on large datasets of unlabelled text, and able to generate novel human-like text. At this point, most LLMs have these characteristics ([Wikipedia](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer), n.d.).

GPT is based on the Transformer architecture, which is a neural network architecture specifically designed for NLP tasks. The Transformer consists of an encoder and a decoder, which work together to process and generate text data. The encoder processes the input text and generates a representation of it, while the decoder uses the representation to generate new text.

The "pre-trained" part of GPT refers to the fact that the model is trained on a large corpus of text data before being fine-tuned for specific NLP tasks. During pre-training, the model learns to predict the next word in a sentence given the preceding words. This task is called language modeling and is a common pre-training objective for NLP models.

Once the model is pre-trained, it can be fine-tuned for specific NLP tasks by training it on a smaller dataset that is specific to the task. During fine-tuning, the model's weights are adjusted to optimize performance on the specific task.

The "generative" part of GPT refers to the fact that the model can generate new text based on a given prompt or context. This is achieved by using the decoder part of the model to generate new text based on the input representation generated by the encoder.

Overall, GPT is a powerful and flexible model for NLP tasks that has achieved state-of-the-art performance on many benchmark datasets. Its ability to generate new text based on a given context has many potential applications, such as chatbots, language translation, and content generation.

<!-- Information gathered from Wikipedia and ChatGPT -->
